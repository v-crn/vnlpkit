{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9303ca1-fa9d-4a35-895c-5f0824d8da3a",
   "metadata": {},
   "source": [
    "# [Hugging Faceを使って事前学習モデルを日本語の感情分析用にファインチューニングしてみた | DevelopersIO](https://dev.classmethod.jp/articles/huggingface-jp-text-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ee565-8f1b-4108-9fc8-324ec87fe5c2",
   "metadata": {},
   "source": [
    "## GPU を認識できるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b234d6ff-c44d-47a8-9f9c-424b2d23526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b0502-3dd7-433d-b19d-c5c8b40c7e3b",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインストール\n",
    "\n",
    "```\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install fugashi\n",
    "!pip install ipadic\n",
    "```\n",
    "\n",
    "## データセット\n",
    "\n",
    "Hugging Face のデータセット：[Hugging Face – The AI community building the future.](https://huggingface.co/datasets)\n",
    "\n",
    "今回は以下のデータセットのうち、日本語のサブセットを使用します。\n",
    "\n",
    "[tyqiangz/multilingual-sentiments · Datasets at Hugging Face](https://huggingface.co/datasets/tyqiangz/multilingual-sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff25583-9c14-4784-a30d-49a793e01a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3d4039e73b4ed896d0e80205ab1dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multilingual-sentiments (/root/.cache/huggingface/datasets/tyqiangz___multilingual-sentiments/japanese/1.0.0/b7cdd8874d82679e59432edf79e074f595c4ad26d2e562eba4fb55f361691b07)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5b4c36086e421f9692511c9ad6b98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43932450-9c8b-4a80-a615-b12cb8249347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bab7dc-af11-41ec-b637-87ad5400c08f",
   "metadata": {},
   "source": [
    "取得したデータセットは以下のようにフォーマットを設定することで、データフレームとして扱うことも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec5d53d-1687-40d3-982f-c0d5f5f3ba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普段使いとバイクに乗るときのブーツ兼用として購入しました。見た目や履き心地は良いです。 しか...</td>\n",
       "      <td>amazon_reviews_multi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>十分な在庫を用意できない販売元も悪いですが、Amazonやら楽⚪︎が転売を認めちゃってるのが...</td>\n",
       "      <td>amazon_reviews_multi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>見た目はかなりおしゃれで気に入りました。2、3回持ち歩いた後いつも通りゼンマイを巻いていたら...</td>\n",
       "      <td>amazon_reviews_multi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>よくある部分での断線はしませんでした ただiphoneとの接続部で接触不良、折れました ip...</td>\n",
       "      <td>amazon_reviews_multi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>プラモデルの塗装剥離に使う為に購入 届いて早速使ってみた 結果 １ヶ月経っても未だに剥離出来...</td>\n",
       "      <td>amazon_reviews_multi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                source  \\\n",
       "0  普段使いとバイクに乗るときのブーツ兼用として購入しました。見た目や履き心地は良いです。 しか...  amazon_reviews_multi   \n",
       "1  十分な在庫を用意できない販売元も悪いですが、Amazonやら楽⚪︎が転売を認めちゃってるのが...  amazon_reviews_multi   \n",
       "2  見た目はかなりおしゃれで気に入りました。2、3回持ち歩いた後いつも通りゼンマイを巻いていたら...  amazon_reviews_multi   \n",
       "3  よくある部分での断線はしませんでした ただiphoneとの接続部で接触不良、折れました ip...  amazon_reviews_multi   \n",
       "4  プラモデルの塗装剥離に使う為に購入 届いて早速使ってみた 結果 １ヶ月経っても未だに剥離出来...  amazon_reviews_multi   \n",
       "\n",
       "   label  \n",
       "0      2  \n",
       "1      2  \n",
       "2      2  \n",
       "3      2  \n",
       "4      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(type=\"pandas\")\n",
    "train_df = dataset[\"train\"][:]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760c043-a23c-4397-be55-59e06d007b14",
   "metadata": {},
   "source": [
    "どうやらamazonのレビューデータが元になって、そちらに対してラベルが付与されているようです。\n",
    "\n",
    "`source` と `label` の内訳を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ef5eb4-8868-4154-bf47-1469d9fe480a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source                label\n",
       "amazon_reviews_multi  0        40000\n",
       "                      1        40000\n",
       "                      2        40000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.value_counts([\"source\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b09a1f-8c2d-48dd-a921-f1c699cf2d2e",
   "metadata": {},
   "source": [
    "各ラベルの意味については、featuresを見れば分かるようになっています。\n",
    "\n",
    "featuresは、各列の値についての詳細が記載してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6702f5-d39d-4391-b2f2-44927ef25e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['positive', 'neutral', 'negative'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca56db-a5c9-46a7-a083-eadefb653061",
   "metadata": {},
   "source": [
    "このように、labelはClassLabelクラスとなっており、0,1,2がそれぞれ'positive','neutral','negative'に割り当てられていることが分かります。\n",
    "\n",
    "ClassLabelクラスには、int2strというメソッドがあり、これでラベル名に変換することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16840a8-35b3-42a6-84b6-a52aabb1983b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mint2str(x)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func\u001b[39m(x, pbar) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     15\u001b[0m     result \u001b[38;5;241m=\u001b[39m label_int2str(\u001b[38;5;28mint\u001b[39m(x))\n\u001b[1;32m     16\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def label_int2str(x):\n",
    "    return dataset[\"train\"].features[\"label\"].int2str(x)\n",
    "\n",
    "\n",
    "# train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\n",
    "\n",
    "\n",
    "def _func(x, pbar) -> list[str]:\n",
    "    result = label_int2str(int(x))\n",
    "    pbar.update(1)\n",
    "    return result\n",
    "\n",
    "\n",
    "input_data = train_df[\"label\"]\n",
    "\n",
    "with tqdm(total=len(input_data)) as pbar:\n",
    "    train_df[\"label_name\"] = np.vectorize(_func)(input_data, pbar)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5a049-92c7-443d-a868-884e3cef560a",
   "metadata": {},
   "source": [
    "最後に、データフレームにしていたフォーマットを元に戻しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976af1c1-3ae3-45c0-b70e-6f45f994a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab22fc0-d3f9-438d-8196-a303d062c651",
   "metadata": {},
   "source": [
    "## モデルの検索\n",
    "\n",
    "データをトークナイザで処理する前に、使用する事前学習モデルを決める必要があります。理由としては、通常事前学習モデルを作成した時と同じトークナイザを使用する必要があるためと考えられます。\n",
    "\n",
    "モデルの検索もHugging Faceのページに準備されており、以下から検索が可能です。\n",
    "\n",
    "<https://huggingface.co/models>\n",
    "\n",
    "この中で、BERTの日本語版を探し、その中が比較的ダウンロード数の多い以下を使用することにします。\n",
    "\n",
    "<https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking>\n",
    "\n",
    "他にも様々な事前学習モデルがありますが、後述するトークナイザの精度などを確認し、問題が無さそうなものを選択しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9247c-a239-4b37-bad4-ba9709dd46b0",
   "metadata": {},
   "source": [
    "## トークナイザの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9d0b2-174b-4b08-b759-e485fd021fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8741f-97d3-4804-9566-bc862c9ccd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\\\n",
    "機械学習のコア部分のロジックを、定型的な実装部分から切り離して\\\n",
    "定義できるようなインターフェースに工夫されています。 \\\n",
    "そのためユーザーは、機械学習のコア部分のロジックの検討に\\\n",
    "集中することができます。\\\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fac040-3020-4e72-b414-c545f0b7cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_encoded = tokenizer(sample_text)\n",
    "print(sample_text_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9333dd-ad09-4f08-9e0c-da00e8e3d0b8",
   "metadata": {},
   "source": [
    "結果はこのように、`input_ids` と `attention_mask` が含まれます。\n",
    "\n",
    "input_idsは数字にエンコードされたトークンで、`attention_mask` は後段のモデルで有効なトークンかどうかを判別するためのマスクです。\n",
    "\n",
    "無効なトークン（例えば、`[PAD]` など）に対しては、`attention_mask` を 0 として処理します。\n",
    "\n",
    "トークナイザの結果は数字にエンコードされているため、トークン文字列を得るには、`convert_ids_to_tokens` を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870af5e-af0a-4350-aea3-382e2c756e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(sample_text_encoded.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d6109-cbe8-4283-ba7a-7610a8eac7d4",
   "metadata": {},
   "source": [
    "結果がこのように得られます。\n",
    "\n",
    "先頭に##が付加されているものは、サブワード分割されているものです。\n",
    "\n",
    "また、系列の開始が[CLS]、系列の終了(実際は複数系列の切れ目)が[SEP]という特殊なトークンとなっています。\n",
    "\n",
    "トークナイザについては以下にも説明があります。\n",
    "\n",
    "[cl-tohoku/bert-base-japanese-whole-word-masking · Hugging Face](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)\n",
    "\n",
    "> The texts are first tokenized by MeCab morphological parser with the IPA dictionary and then split into subwords by the WordPiece algorithm. The vocabulary size is 32000.\n",
    "\n",
    "トークン化にIPA辞書を使ったMecabが使用され、サブワード分割にはWordPieceアルゴリズムが使われているようです。\n",
    "\n",
    "その他、文字列を再構成するには、convert_tokens_to_stringを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ff71a-08a3-4883-8a10-3b0ed66f63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4705b83-5a16-46a9-a5d9-a9c01469f5ef",
   "metadata": {},
   "source": [
    "## データセット全体のトークン化\n",
    "\n",
    "データセット全体に処理を適用するには、バッチ単位で処理する関数を定義し、mapを使って実施します。\n",
    "\n",
    "- `padding=True` でバッチ内の最も長い系列長に合うようpaddingする処理を有効にします。\n",
    "- `truncation=True` で、後段のモデルが対応する最大コンテキストサイズ以上を切り捨てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b912e4-3341-4fb1-a3e9-0d423b88746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a0966-bf82-440f-bb89-a4278e5059c2",
   "metadata": {},
   "source": [
    "参考までにモデルが対応する最大コンテキストサイズは、以下で確認ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db64115-d6f3-42c3-b119-c5fd470fc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4da881-6097-4627-9bc3-75d8db4e2cb5",
   "metadata": {},
   "source": [
    "これをデータセット全体に適用します。\n",
    "\n",
    "- `batched=True` によりバッチ化され、`batch_size=None` により全体が1バッチとなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fb0de-4f2a-426c-8b63-5003565bdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e1475-e611-411f-affc-e1a4b80d778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e25c45-9d24-4255-be41-512a4977e999",
   "metadata": {},
   "source": [
    "データセット全体に適用され、カラムが追加されていることが分かります。\n",
    "\n",
    "token_types_idは今回使用しませんが、複数の系列がある場合に使用されます。(詳細は下記を参照)\n",
    "\n",
    "[Glossary](https://huggingface.co/docs/transformers/glossary#token-type-ids)\n",
    "\n",
    "サンプル単位で結果を確認したい場合は、データフレームなどを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf266e-b3d2-41bb-8975-4b0707f5d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_encoded = dataset_encoded[\"train\"][0]\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        sample_encoded[\"input_ids\"],\n",
    "        sample_encoded[\"attention_mask\"],\n",
    "        tokenizer.convert_ids_to_tokens(sample_encoded[\"input_ids\"]),\n",
    "    ],\n",
    "    [\"input_ids\", \"attention_mask\", \"tokens\"],\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e4745-7801-47b5-9071-9e37221a53e6",
   "metadata": {},
   "source": [
    "## 分類器の実現方法\n",
    "\n",
    "テキスト分類のためにはここから、BERTモデルの後段に分類用のヘッドを接続する必要があります。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/cm-nakamura-shogo/devio-image/main/huggingface-jp-text-classification/huggingface-jp-text-classification-2.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "接続後、テキスト分類を学習する方法に大きく２種類あります。\n",
    "\n",
    "接続した分類用ヘッドのみを学習\n",
    "BERTを含むモデル全体を学習(fine-tuning)\n",
    "前者は高速な学習が可能でGPUなどが利用できない場合に選択肢になり、後者の方がよりタスクに特化できるので高精度となります。\n",
    "\n",
    "本記事では後者のfine-tuningする方法で実装していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470188ea-a344-4469-a53c-aee1b846991e",
   "metadata": {},
   "source": [
    "## 分類器の実装\n",
    "\n",
    "今回のようなテキストを系列単位で分類するタスクには、既にそれ専用のクラスが準備されており、以下で構築が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71c0fe-dc44-4007-b5c3-408d70401197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 3\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79062185-9dcc-4525-af26-816be916dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=num_labels\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7189a-415f-4765-bd60-f5989a4b3ff0",
   "metadata": {},
   "source": [
    "## トレーニングの準備\n",
    "\n",
    "学習時に性能指標を与える必要があるため、それを関数化して定義しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9be31-0448-4136-977d-4d34c04adf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3ad2b-9f6f-4df4-8339-f491109b288a",
   "metadata": {},
   "source": [
    "こちらは `EvalPrediction` オブジェクトをうけとる形で実装します。\n",
    "\n",
    "`EvalPrediciton` オブジェクトは、`predictions` と `label_ids` という属性を持つ `named_tuple` です。\n",
    "\n",
    "そして学習用のパラメータを `TrainingArguments` クラスを用いて設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f8d8e-74f7-4cd5-8c88-9abbb999fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = \"sample-text-classification-bert\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a1a03-3f3a-4587-84c4-dc482044532e",
   "metadata": {},
   "source": [
    "## トレーニングの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f665e6-bfc1-44fc-9959-facd6785e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314405f6-e711-4a1a-bd4e-308d3e9b2bd4",
   "metadata": {},
   "source": [
    "## 推論テスト\n",
    "\n",
    "推論結果は `predict` により得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c41d5d-5d50-4b74-ae85-5f29a7da856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds_output = trainer.predict(dataset_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08d273-4a90-4371-810b-0800a47b2e23",
   "metadata": {},
   "source": [
    "これを混同行列で可視化してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2dde3-9899-4053-a730-8314f5a86e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = np.array(dataset_encoded[\"validation\"][\"label\"])\n",
    "labels = dataset_encoded[\"train\"].features[\"label\"].names\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d5693-bf8a-4e05-9eb5-68bc720425a0",
   "metadata": {},
   "source": [
    "positive, negativeについては9割以上で正解できていますが、neutralの判別が少し難しくなっていそうです。\n",
    "またpositiveをnegativeに間違えたり、negativeをpositiveに間違えたりすることは少ないようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e9873-a738-4603-9725-7c368188ec18",
   "metadata": {},
   "source": [
    "## モデル保存\n",
    "\n",
    "保存前にラベル情報を設定しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4aa6cc-cf86-4fe1-836b-a6b0292120b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {}\n",
    "for i in range(dataset[\"train\"].features[\"label\"].num_classes):\n",
    "    id2label[i] = dataset[\"train\"].features[\"label\"].int2str(i)\n",
    "\n",
    "label2id = {}\n",
    "for i in range(dataset[\"train\"].features[\"label\"].num_classes):\n",
    "    label2id[dataset[\"train\"].features[\"label\"].int2str(i)] = i\n",
    "\n",
    "trainer.model.config.id2label = id2label\n",
    "trainer.model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847df5d-c08f-4342-8f99-b775a3ba411d",
   "metadata": {},
   "source": [
    "`save_model` で保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d1112-726c-4126-a163-4180895c9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"work/nlp_tasks/text_classification/Hugging_Faceを使って事前学習モデルを日本語の感情分析用にファインチューニングしてみた/sample-text-classification-bert\"\n",
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d76f0f-44c9-43ed-823f-ab32340b184c",
   "metadata": {},
   "source": [
    "保存結果は以下のようなファイル構成となります。\n",
    "\n",
    "```txt\n",
    "sample-text-classification-bert\n",
    "├── config.json\n",
    "├── pytorch_model.bin\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer_config.json\n",
    "├── training_args.bin\n",
    "└── vocab.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb6dc3-01e2-40dc-a73b-59117234b16e",
   "metadata": {},
   "source": [
    "モデルやトークナイザの設定ファイル、そしてメインのモデルは pytorch_model.bin として保存されているようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f0735-5268-48e3-854d-5165c26de733",
   "metadata": {},
   "source": [
    "## ロードして推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b426b-4b6c-48b1-896f-176e5e2fe3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b5c9b-d6cd-48f5-9d2e-ace1329ea6d8",
   "metadata": {},
   "source": [
    "サンプルテキストを推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944c102-2a5e-4106-9444-bc5e6f3f1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = new_tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "new_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = new_model(\n",
    "        inputs[\"input_ids\"].to(device),\n",
    "        inputs[\"attention_mask\"].to(device),\n",
    "    )\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d65581-9d28-4fec-91a8-582da70e48f7",
   "metadata": {},
   "source": [
    "logitsを推論ラベルに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54da2fa-42f1-4c49-823c-8b854e8e9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(outputs.logits.to(\"cpu\").detach().numpy().copy(), axis=1)\n",
    "\n",
    "\n",
    "def id2label(x):\n",
    "    return new_model.config.id2label[x]\n",
    "\n",
    "\n",
    "y_dash = [id2label(x) for x in y_preds]\n",
    "y_dash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

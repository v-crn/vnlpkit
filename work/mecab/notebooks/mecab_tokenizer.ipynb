{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bf6c270e-da2e-43f0-8a5f-439020d8757e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5dc37962-05cf-421e-907a-d8e85487534b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "cabce5b2-e02d-4d85-8509-9868c7bdf555",
      "metadata": {},
      "outputs": [],
      "source": [
        "from work.config.path.livedoor_news_corpus_paths import LivedoorNewsCorpusPaths\n",
        "from work.mecab.config.mecab_dic_paths import MecabDicPaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "f7a4811b-a1ed-485c-8ce0-4144742086e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from typing import Iterable\n",
        "\n",
        "import demoji\n",
        "import neologdn\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_neologdn: bool = True,\n",
        "        remove_url: bool = True,\n",
        "        remove_emoji: bool = True,\n",
        "        remove_thousand_separator: bool = True,\n",
        "        replace_digit: bool = False,\n",
        "        remove_full_width_symbol: bool = True,\n",
        "        remove_half_width_symbol: bool = True,\n",
        "        use_unicode_normalization: bool = True,\n",
        "    ) -> None:\n",
        "        self.use_neologdn = use_neologdn\n",
        "        self.remove_url = remove_url\n",
        "        self.remove_emoji = remove_emoji\n",
        "        self.remove_thousand_separator = remove_thousand_separator\n",
        "        self.replace_digit = replace_digit\n",
        "        self.remove_full_width_symbol = remove_full_width_symbol\n",
        "        self.remove_half_width_symbol = remove_half_width_symbol\n",
        "        self.use_unicode_normalization = use_unicode_normalization\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        text: str,\n",
        "        use_neologdn: bool = True,\n",
        "        remove_url: bool = True,\n",
        "        remove_emoji: bool = True,\n",
        "        remove_thousand_separator: bool = True,\n",
        "        replace_digit: bool = False,\n",
        "        remove_full_width_symbol: bool = True,\n",
        "        remove_half_width_symbol: bool = True,\n",
        "        use_unicode_normalization: bool = True,\n",
        "    ) -> str:\n",
        "        self.use_neologdn = use_neologdn\n",
        "        self.remove_url = remove_url\n",
        "        self.remove_emoji = remove_emoji\n",
        "        self.remove_thousand_separator = remove_thousand_separator\n",
        "        self.replace_digit = replace_digit\n",
        "        self.remove_full_width_symbol = remove_full_width_symbol\n",
        "        self.remove_half_width_symbol = remove_half_width_symbol\n",
        "        self.use_unicode_normalization = use_unicode_normalization\n",
        "\n",
        "        # URL の削除（空白文字に置換）\n",
        "        # [Note] `separator` の前後に空白文字がないと、separator も URL の一部と見做されて消されることがあるので注意\n",
        "        if self.remove_url:\n",
        "            regex = (\n",
        "                r\"(https?|ftp?|http?)(:\\/\\/[-_\\.!~*\\’()a-zA-Z0-9;\\/?:\\@ &=\\+\\$,%#]+)\"\n",
        "            )\n",
        "            text = re.sub(regex, \" \", text)\n",
        "\n",
        "        # 絵文字の削除（空白文字に置換）\n",
        "        if remove_emoji:\n",
        "            text = demoji.replace(string=text, repl=\" \")\n",
        "\n",
        "        # 数字の桁区切り記号 (,) の削除\n",
        "        if remove_thousand_separator:\n",
        "            text = re.sub(r\"(\\d)([,.])(\\d+)\", r\"\\1\\3\", text)\n",
        "\n",
        "        # 数字を '0' に置換\n",
        "        if replace_digit:\n",
        "            text = re.sub(r\"\\d+\", \"0\", text)\n",
        "\n",
        "        # 半角記号を削除\n",
        "        if remove_half_width_symbol:\n",
        "            text = re.sub(r\"[!-/:-@[-`{-~]\", \" \", text)\n",
        "\n",
        "        # 全角記号を削除（ここでは 0x25A0 - 0x266F のブロックのみを削除）\n",
        "        if remove_full_width_symbol:\n",
        "            text = re.sub(\"[■-♯]\", \" \", text)\n",
        "\n",
        "        # 全角・半角の統一と重ね表現の削除\n",
        "        if self.use_neologdn:\n",
        "            text = neologdn.normalize(text)\n",
        "\n",
        "        # Unicode 正規化\n",
        "        if use_unicode_normalization:\n",
        "            text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def run_all(\n",
        "        self,\n",
        "        documents: Iterable[str],\n",
        "        use_neologdn: bool = True,\n",
        "        remove_url: bool = True,\n",
        "        remove_emoji: bool = True,\n",
        "        remove_thousand_separator: bool = True,\n",
        "        replace_digit: bool = False,\n",
        "        remove_full_width_symbol: bool = True,\n",
        "        remove_half_width_symbol: bool = True,\n",
        "        use_unicode_normalization: bool = True,\n",
        "    ) -> list[str]:\n",
        "        def _func(text: str, pbar) -> list[str]:\n",
        "            filtered_text = self.run(\n",
        "                text,\n",
        "                use_neologdn=use_neologdn,\n",
        "                remove_url=remove_url,\n",
        "                remove_emoji=remove_emoji,\n",
        "                remove_thousand_separator=remove_thousand_separator,\n",
        "                replace_digit=replace_digit,\n",
        "                remove_full_width_symbol=remove_full_width_symbol,\n",
        "                remove_half_width_symbol=remove_half_width_symbol,\n",
        "                use_unicode_normalization=use_unicode_normalization,\n",
        "            )\n",
        "            pbar.update(1)\n",
        "            return filtered_text\n",
        "\n",
        "        with tqdm(total=len(documents)) as pbar:\n",
        "            filtered_documents = np.vectorize(_func)(documents, pbar)\n",
        "\n",
        "        return filtered_documents\n",
        "\n",
        "    def run_all_with_sep(\n",
        "        self,\n",
        "        documents: Iterable[str],\n",
        "        separator: str = \"üßäö\",\n",
        "        use_neologdn: bool = True,\n",
        "        remove_url: bool = True,\n",
        "        remove_emoji: bool = True,\n",
        "        remove_thousand_separator: bool = True,\n",
        "        replace_digit: bool = False,\n",
        "        remove_full_width_symbol: bool = True,\n",
        "        remove_half_width_symbol: bool = True,\n",
        "        use_unicode_normalization: bool = True,\n",
        "    ) -> list[str]:\n",
        "        \"\"\"\n",
        "        `run_all_with_sep()` は `run_all()` よりも高速ですが、データによってはうまく separator が機能しないことがあります。\n",
        "        適切な separator を見つけるのが難しい場合、 `run_all()` の使用を推奨します。\n",
        "        \"\"\"\n",
        "        tmp_separator = separator\n",
        "        text: str = tmp_separator.join(documents)\n",
        "        filtered_documents_text = self.run(\n",
        "            text,\n",
        "            use_neologdn=use_neologdn,\n",
        "            remove_url=remove_url,\n",
        "            remove_emoji=remove_emoji,\n",
        "            remove_thousand_separator=remove_thousand_separator,\n",
        "            replace_digit=replace_digit,\n",
        "            remove_full_width_symbol=remove_full_width_symbol,\n",
        "            remove_half_width_symbol=remove_half_width_symbol,\n",
        "            use_unicode_normalization=use_unicode_normalization,\n",
        "        )\n",
        "        filtered_documents = filtered_documents_text.split(separator)\n",
        "        # diff = len(documents) - len(filtered_documents)\n",
        "        # assert diff == 0, f\"documents length must be equal to filtered_documents. \\ndiff: {diff}\"\n",
        "\n",
        "        return filtered_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "fb0681eb-b564-4dcb-bd7c-e46e18df0129",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import MeCab\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "class MecabTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dict_path: str | None = None,\n",
        "    ) -> None:\n",
        "        self.tagger = (\n",
        "            MeCab.Tagger() if dict_path is None else MeCab.Tagger(f\"-d {dict_path}\")\n",
        "        )\n",
        "        self.kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
        "        self.stop_words = None\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        text: str,\n",
        "        target_pos_0_list: list[str] | None = [\"名詞\", \"動詞\", \"形容詞\"],\n",
        "        remove_hiragana_only: bool = False,\n",
        "        lower_letter_case: bool = True,\n",
        "        stop_words: list[str] | None = None,\n",
        "    ) -> str:\n",
        "        self.stop_words = stop_words if stop_words is not None else []\n",
        "\n",
        "        # 分かち書き\n",
        "        parsed_text = self.tagger.parse(text)\n",
        "        parsed_lines = parsed_text.split(\"\\n\")[:-2]\n",
        "        surfaces = [l.split(\"\\t\")[0] for l in parsed_lines]\n",
        "        features = [l.split(\"\\t\")[1] for l in parsed_lines]\n",
        "\n",
        "        # 原型を取得\n",
        "        bases = [f.split(\",\")[6] for f in features]\n",
        "\n",
        "        # 各単語を原型に変換する\n",
        "        token_list = [b if b != \"*\" else s for s, b in zip(surfaces, bases)]\n",
        "\n",
        "        # 品詞の絞り込み\n",
        "        if (target_pos_0_list is not None) and (len(target_pos_0_list) > 0):\n",
        "            pos = [f.split(\",\")[0] for f in features]\n",
        "            token_list = [t for t, p in zip(token_list, pos) if (p in target_pos_0_list)]\n",
        "\n",
        "        _token_list = []\n",
        "        for token in token_list:\n",
        "            # stopwords に含まれていれば除去\n",
        "            if token in self.stop_words:\n",
        "                continue\n",
        "\n",
        "            # ひらがなのみの単語を除く\n",
        "            if remove_hiragana_only and self.kana_re.match(token):\n",
        "                continue\n",
        "\n",
        "            # アルファベットを小文字に統一\n",
        "            token = token.lower() if lower_letter_case else token\n",
        "\n",
        "            _token_list.append(token)\n",
        "\n",
        "        # 半角スペースを挟んで結合\n",
        "        tokenized_text = \" \".join(_token_list)\n",
        "\n",
        "        # 再度ユニコード正規化\n",
        "        tokenized_text = unicodedata.normalize(\"NFKC\", tokenized_text)\n",
        "\n",
        "        return tokenized_text\n",
        "\n",
        "    def run_all(\n",
        "        self,\n",
        "        documents: list[str],\n",
        "        target_pos_0_list: list[str] | None = [\"名詞\", \"動詞\", \"形容詞\"],\n",
        "        remove_hiragana_only: bool = False,\n",
        "        lower_letter_case: bool = True,\n",
        "        stop_words: list[str] | None = None,\n",
        "    ) -> list[str]:\n",
        "        def _func(text: str, pbar) -> list[str]:\n",
        "            tokenized_text = self.run(\n",
        "                text,\n",
        "                target_pos_0_list=target_pos_0_list,\n",
        "                remove_hiragana_only=remove_hiragana_only,\n",
        "                lower_letter_case=lower_letter_case,\n",
        "                stop_words=stop_words,\n",
        "            )\n",
        "            pbar.update(1)\n",
        "            return tokenized_text\n",
        "\n",
        "        with tqdm(total=len(documents)) as pbar:\n",
        "            tokenized_documents = np.vectorize(_func)(documents, pbar)\n",
        "\n",
        "        return tokenized_documents\n",
        "\n",
        "    def run_all_with_sep(\n",
        "        self,\n",
        "        documents: list[str],\n",
        "        separator: str = \"üßäö\",\n",
        "        target_pos_0_list: list[str] | None = [\"名詞\", \"動詞\", \"形容詞\"],\n",
        "        remove_hiragana_only: bool = False,\n",
        "        lower_letter_case: bool = True,\n",
        "        stop_words: list[str] | None = None,\n",
        "    ) -> list[str]:\n",
        "        \"\"\"\n",
        "        `run_all_with_sep()` は `run_all()` よりも高速ですが、データによってはうまく separator が機能しないことがあります。\n",
        "        適切な separator を見つけるのが難しい場合、 `run_all()` の使用を推奨します。\n",
        "        \"\"\"\n",
        "        tmp_separator = separator\n",
        "        text = tmp_separator.join(documents)\n",
        "        tokenized_documents_text = self.run(\n",
        "            text,\n",
        "            target_pos_0_list=target_pos_0_list,\n",
        "            remove_hiragana_only=remove_hiragana_only,\n",
        "            lower_letter_case=lower_letter_case,\n",
        "            stop_words=stop_words,\n",
        "        )\n",
        "        tokenized_documents = tokenized_documents_text.split(separator)\n",
        "        # diff = len(documents) - len(tokenized_documents)\n",
        "        # assert diff == 0, f\"documents length must be equal to filtered_documents. \\ndiff: {diff}\"\n",
        "\n",
        "        return tokenized_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8071f6b3-8ccb-414e-b1a3-e7917d967c0d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元：独女通信http:/...</td>\n",
              "      <td>もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...</td>\n",
              "      <td>携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <td>dokujo-tsushin</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category_label</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                0  \\\n",
              "text            自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元：独女通信http:/...   \n",
              "category                                           dokujo-tsushin   \n",
              "category_label                                                  0   \n",
              "\n",
              "                                                                1  \\\n",
              "text            もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...   \n",
              "category                                           dokujo-tsushin   \n",
              "category_label                                                  0   \n",
              "\n",
              "                                                                2  \n",
              "text            携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...  \n",
              "category                                           dokujo-tsushin  \n",
              "category_label                                                  0  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# データ読み込み\n",
        "filepath = CorpusPaths.LIVEDOOR_NEWS_CORPUS_PATH\n",
        "df = pd.read_csv(filepath)\n",
        "df.head(3).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "465e1acf-c15b-4955-adf1-85c23f7a2973",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7376 entries, 0 to 7375\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   text            7376 non-null   object\n",
            " 1   category        7376 non-null   object\n",
            " 2   category_label  7376 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 173.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bffd9ccc-b9eb-4ed5-baa5-32c909859ace",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LivedoorNewsColumns:\n",
        "    TEXT = \"text\"\n",
        "    CATEGORY = \"category\"\n",
        "    CATEGORY_LABEL = \"category_label\"\n",
        "    TOKEN = \"token\"\n",
        "    FILTERED_TEXT = \"filtered_text\"\n",
        "    FILTERED_TEXT_TOKEN = \"filtered_text_token\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4015ae11-0050-41f0-bc0d-d49d2dc52078",
      "metadata": {},
      "source": [
        "## テキストの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "9474284c-221e-469a-ac89-1b01b39d0952",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41e3bd0fb389479785196f41f02eb56c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 24.2 s, sys: 84.3 ms, total: 24.3 s\n",
            "Wall time: 23.9 s\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "# text_preprocessor = TextPreprocessor()\n",
        "\n",
        "# _df = df.sample(500)\n",
        "# documents = _df[LivedoorNewsColumns.TEXT].to_numpy()\n",
        "# filtered = text_preprocessor.run_all(documents)\n",
        "# diff = len(filtered) - len(documents)\n",
        "# assert diff == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f1413bf4-a66f-494c-aca7-c772e60af463",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'12日、日本陸上競技連盟は、ロンドン五輪マラソンの日本代表メンバーを発表した。男子は藤原新（東京陸協）、山本亮（佐川急便）、中本健太郎（安川電機）、補欠・堀端宏行（旭化成）となり、川内優輝（埼玉県庁）は落選。女子は、重友梨佐（天満屋）、木崎良子（ダイハツ）、尾崎好美（第一生命）、補欠・赤羽有紀子（ホクレン）が選ばれた。'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6baefbe4-4080-4f35-ab1a-00e667c29b2d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'12日、日本陸上競技連盟は、ロンドン五輪マラソンの日本代表メンバーを発表した。男子は藤原新(東京陸協)、山本亮(佐川急便)、中本健太郎(安川電機)、補欠・堀端宏行(旭化成)となり、川内優輝(埼玉県庁)は落選。女子は、重友梨佐(天満屋)、木崎良子(ダイハツ)、尾崎好美(第一生命)、補欠・赤羽有紀子(ホクレン)が選ばれた。'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filtered[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0cd0f3cc-b77f-460f-b756-6a852b8ea600",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 36s, sys: 971 ms, total: 5min 37s\n",
            "Wall time: 5min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "text_preprocessor = TextPreprocessor()\n",
        "\n",
        "documents = df[LivedoorNewsColumns.TEXT].to_numpy()\n",
        "df[LivedoorNewsColumns.FILTERED_TEXT] = text_preprocessor.run_all(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4cafaa5-5372-4ae7-a850-079288aa63d9",
      "metadata": {},
      "source": [
        "## 形態素解析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "64b40ef6-6364-43f9-8f67-d54483a92cf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def _tokenize(text: str, pbar) -> list[str]:\n",
        "#     result = tokenizer.run(text)\n",
        "#     pbar.update(1)\n",
        "\n",
        "#     return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "25afebf7-daf5-4673-93be-1bd6ac2e3b08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%time\n",
        "# _df = df.sample(3)\n",
        "# tokenizer = MecabTokenizer()\n",
        "# documents = _df[LivedoorNewsColumns.FILTERED_TEXT].to_numpy()\n",
        "\n",
        "# with tqdm(total=len(documents)) as pbar:\n",
        "#     result = np.vectorize(_tokenize)(documents, pbar)\n",
        "#     _df[LivedoorNewsColumns.FILTERED_TEXT_TOKEN] = result\n",
        "\n",
        "# print(f\"{documents[0]}\\n\")\n",
        "# print(result[0])\n",
        "# print()\n",
        "\n",
        "# diff = len(result) - len(documents)\n",
        "# print(diff)\n",
        "# assert diff == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "1be89712-6cc9-4a61-ae00-714e861b4f9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元:独女通信\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1962745934c943908ba10acfe9e3d8f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7376 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 15s, sys: 1.46 s, total: 1min 17s\n",
            "Wall time: 1min 16s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'自由 転載 引用 可能 ディレクトリ 記事 ファイル 内容 提供元 独女通信'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "tokenizer = MecabTokenizer()\n",
        "documents = df[LivedoorNewsColumns.FILTERED_TEXT].to_numpy()\n",
        "print(documents[0])\n",
        "\n",
        "result = tokenizer.run_all(documents)\n",
        "df[LivedoorNewsColumns.FILTERED_TEXT_TOKEN] = result\n",
        "result[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "d39735d4-aed5-4c80-a623-d445576d6a64",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_label</th>\n",
              "      <th>filtered_text</th>\n",
              "      <th>filtered_text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元：独女通信http:/...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "      <td>0</td>\n",
              "      <td>自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元:独女通信</td>\n",
              "      <td>自由 転載 引用 可能 ディレクトリ 記事 ファイル 内容 提供元 独女通信</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        category  \\\n",
              "0  自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元：独女通信http:/...  dokujo-tsushin   \n",
              "\n",
              "   category_label                             filtered_text  \\\n",
              "0               0  自由に転載・引用が可能です。このディレクトリの記事ファイル内容の提供元:独女通信   \n",
              "\n",
              "                      filtered_text_token  \n",
              "0  自由 転載 引用 可能 ディレクトリ 記事 ファイル 内容 提供元 独女通信  "
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "7bfcc289-8bb1-4072-9b77-8a13b5ab3377",
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = LivedoorNewsCorpusPaths.LIVEDOOR_NEWS_CORPUS_PRP_PATH\n",
        "df.to_csv(filepath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe0eef8-8798-4bbf-980a-2e168a799ed7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

import json
from typing import Iterable
from collections import Counter

import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud


class WordCloudGenerator:
    def __init__(self) -> None:
        self.model = None
        self.tfidf_vectorizer = None

    def generate(
        self,
        tokenized_text: str | None = None,
        documents: Iterable[str] | None = None,
        n_token: int | None = None,
        tfidf_dict: dict[str, float] | None = None,
        tfidf_json_path: str | None = None,
        fig_path: str | None = None,
        font_path: str | None = None,
        title: str | None = None,
        width: int = 400,
        height: int = 200,
        prefer_horizontal: float = 1.0,
        min_font_size: int = 4,
        stopwords: set[str] | None = None,
        random_state: int | None = 0,
        background_color: str = "black",
        max_font_size: int | None = None,
        font_step: int = 1,
        mode: str = "RGB",
        relative_scaling: float | str = "auto",
        regexp: str | None = r"[\w']+",
        collocations: bool = True,
        colormap: str | None = None,
        normalize_plurals=True,
        contour_width=0,
        contour_color="black",
        repeat=False,
        include_numbers=True,
        min_word_length=0,
        collocation_threshold=30,
    ) -> dict:
        """
        --- Main Inputs ---

        By default, WordCloud is generated by word frequency.
        Please specify one of the following arguments to generate WordCloud based on TF-IDF:

          - tokenized_text & documents
          - tfidf_dict

        -------------------

        --- Stop Words ---

        - stopwords is used only when calling WordCloud.generate_from_text()

        ------------------
        """
        result_dict = {}
        model = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            prefer_horizontal=prefer_horizontal,
            min_font_size=min_font_size,
            stopwords=stopwords,
            random_state=random_state,
            background_color=background_color,
            max_font_size=max_font_size,
            font_step=font_step,
            mode=mode,
            relative_scaling=relative_scaling,
            regexp=regexp,
            collocations=collocations,
            colormap=colormap,
            normalize_plurals=normalize_plurals,
            contour_width=contour_width,
            contour_color=contour_color,
            repeat=repeat,
            include_numbers=include_numbers,
            min_word_length=min_word_length,
            collocation_threshold=collocation_threshold,
        )
        self.model = model
        tfidf_vectorizer = self.tfidf_vectorizer

        if tfidf_dict is not None:
            tfidf_dict = {k: v for k, v in tfidf_dict.items() if (k not in stopwords) and (v > 0)}
            generated_wc = model.generate_from_frequencies(tfidf_dict)

        elif (documents is not None) and (tokenized_text is not None):
            tfidf_vectorizer = TfidfVectorizer(
                token_pattern="(?u)\\b\\w+\\b", norm=None
            )
            tfidf_vectorizer.fit(documents)
            tfidf_vec = tfidf_vectorizer.transform([tokenized_text]).toarray()[0]
            tfidf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vec))
            tfidf_dict = {k: v for k, v in tfidf_dict.items() if (k not in stopwords) and (v > 0)}

            generated_wc = model.generate_from_frequencies(tfidf_dict)

            if tfidf_json_path is not None:
                f = open(tfidf_json_path, "w")
                json.dump(tfidf_dict, f)
                f.close()

            self.tfidf_vectorizer = tfidf_vectorizer

        elif tokenized_text is not None:
            tokenized_text = [token for token in tokenized_text.split(" ") if token not in stopwords]
            word_frequencies = Counter(tokenized_text)
            if n_token is not None:
                word_frequencies = dict(word_frequencies.most_common(n_token))

            generated_wc = model.generate_from_frequencies(word_frequencies)

        else:
            raise ValueError(
                "The argument 'tokenized_text' or ('tokenized_text' & 'documents') or 'tfidf_dict' must be specified."
            )

        plt.imshow(generated_wc)
        plt.title(title)
        plt.axis("off")
        plt.show()

        if fig_path is not None:
            model.to_file(fig_path)

        result_dict["wordcloud_model"] = generated_wc
        result_dict["tfidf_dict"] = tfidf_dict
        result_dict["tfidf_vectorizer"] = tfidf_vectorizer

        return result_dict

    def generate_from_text(
        self,
        tokenized_text: str | None = None,
        **args,
    ) -> dict:
        return self.generate(tokenized_text, **args)

    def generate_from_tfidf_for_text_and_documents(
        self,
        tokenized_text: str | None = None,
        documents: Iterable[str] | None = None,
        **args,
    ) -> dict:
        return self.generate(tokenized_text, documents, **args)

    def generate_from_tfidf_dict(
        self,
        tfidf_dict: dict[str, float] | None = None,
        **args,
    ) -> dict:
        return self.generate(tfidf_dict, **args)

import json
from collections import Counter
from typing import Iterable, Optional, Dict, Set

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import japanize_matplotlib
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud


class WordCloudGenerator:
    def __init__(self) -> None:
        self.model = None
        self.tfidf_vectorizer = None

    def generate(
        self,
        tokenized_text: Optional[str] = None,
        documents: Optional[Iterable[str]] = None,
        n_token: Optional[int] = None,
        tfidf_dict: Optional[Dict[str, float]] = None,
        freq_dict_path: Optional[str] = None,
        fig_path: Optional[str] = None,
        font_path: Optional[str] = "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc",
        title: Optional[str] = None,
        width: int = 400,
        height: int = 200,
        prefer_horizontal: float = 1.0,
        min_font_size: int = 4,
        stopwords: Optional[Set[str]] = None,
        random_state: Optional[int] = 0,
        background_color: str = "black",
        max_font_size: Optional[int] = None,
        font_step: int = 1,
        mode: str = "RGB",
        relative_scaling: Optional[float] = "auto",
        regexp: Optional[str] = r"[\w']+",
        sep: str = " ",
        collocations: bool = True,
        colormap: Optional[str] = None,
        normalize_plurals=True,
        contour_width=0,
        contour_color="black",
        repeat=False,
        include_numbers=True,
        min_word_length=0,
        collocation_threshold=30,
    ) -> Dict[str, int]:
        """
        --- Main Inputs ---

        By default, WordCloud is generated by word frequency.
        Please specify one of the following arguments to generate WordCloud based on TF-IDF:

          - tokenized_text & documents
          - tfidf_dict

        日本語を扱う場合は font_path に日本語フォントを指定しないと文字化けします。

        ------------------
        """
        stopwords = list(set(stopwords)) if stopwords is not None else []
        result_dict = {}
        model = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            prefer_horizontal=prefer_horizontal,
            min_font_size=min_font_size,
            stopwords=stopwords,
            random_state=random_state,
            background_color=background_color,
            max_font_size=max_font_size,
            font_step=font_step,
            mode=mode,
            relative_scaling=relative_scaling,
            regexp=regexp,
            collocations=collocations,
            colormap=colormap,
            normalize_plurals=normalize_plurals,
            contour_width=contour_width,
            contour_color=contour_color,
            repeat=repeat,
            include_numbers=include_numbers,
            min_word_length=min_word_length,
            collocation_threshold=collocation_threshold,
        )
        self.model = model
        tfidf_vectorizer = self.tfidf_vectorizer

        if tfidf_dict is not None:
            tfidf_dict = {
                k: v for k, v in tfidf_dict.items() if (k not in stopwords) and (v > 0)
            }
            freq_dict = tfidf_dict
            generated_wc = model.generate_from_frequencies(freq_dict)

        elif (documents is not None) and (tokenized_text is not None):
            tfidf_vectorizer = TfidfVectorizer(
                token_pattern="(?u)\\b\\w+\\b", norm=None
            )
            tfidf_vectorizer.fit(documents)
            tfidf_vec = tfidf_vectorizer.transform([tokenized_text]).toarray()[0]
            tfidf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vec))
            tfidf_dict = {
                k: v for k, v in tfidf_dict.items() if (k not in stopwords) and (v > 0)
            }
            self.tfidf_vectorizer = tfidf_vectorizer
            freq_dict = tfidf_dict
            generated_wc = model.generate_from_frequencies(freq_dict)

        elif tokenized_text is not None:
            tokenized_text = [
                token for token in tokenized_text.split(sep) if token not in stopwords
            ]
            freq_dict = Counter(tokenized_text)
            if n_token is not None:
                freq_dict = dict(freq_dict.most_common(n_token))

            freq_dict = dict(freq_dict)
            generated_wc = model.generate_from_frequencies(freq_dict)

        else:
            raise ValueError(
                "The argument 'tokenized_text' or ('tokenized_text' & 'documents') or 'tfidf_dict' must be specified."
            )

        if freq_dict_path is not None:
            with open(freq_dict_path, "w") as f:
                json.dump(
                    freq_dict,
                    f,
                    ensure_ascii=False,
                    indent=4,
                    sort_keys=True,
                    separators=(",", ": "),
                )

        plt.imshow(generated_wc)
        plt.title(title)
        plt.axis("off")
        plt.show()

        if fig_path is not None:
            model.to_file(fig_path)

        result_dict["wordcloud_model"] = generated_wc
        result_dict["freq_dict"] = freq_dict
        result_dict["tfidf_vectorizer"] = tfidf_vectorizer

        return result_dict

    def generate_from_text(
        self,
        tokenized_text: Optional[str] = None,
        **args,
    ) -> Dict:
        return self.generate(tokenized_text, **args)

    def generate_from_tfidf_for_text_and_documents(
        self,
        tokenized_text: Optional[str] = None,
        documents: Optional[Iterable[str]] = None,
        **args,
    ) -> Dict:
        return self.generate(tokenized_text, documents, **args)

    def generate_from_tfidf_dict(
        self,
        tfidf_dict: Optional[Dict[str, float]] = None,
        **args,
    ) -> Dict:
        return self.generate(tfidf_dict, **args)
